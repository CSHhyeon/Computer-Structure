2강 

컴퓨터는 0하고 1만 이해할 수 있다.

그러면 0과 1이 아닌 숫자와 문자는 어떻게 이해하는데?

 

숫자를 표현하는 방법
1. 정보단위


2. 워드 

- CPU가 한번에 처리할 수 있는 정보의 크기 단위

ex) 현재 컴퓨터는 보통 32비트, 64비트 => 1워드 = 32bit or 64bit

 

3, 2의 보수법

- 모든 0과 1을 뒤집고 1을 더한 값 

=> 플래그 레지스터가 부호 표식.


 

이진법의 단점? 너무 길어진다 => 해결법? 16진법 

서로 변환이 쉬워서 많이 사용됨.

 

문자를 표현하는 방법
1. 문자집합

- 컴퓨터가 인식하고 표현할 수 있는 문자의 모음 

 

2. 인코딩(코드화)

- 문자 => 0,1로 변환

 

3. 디코딩(코드해석)

- 0,1 => 문자로 변환 

 

문자집합 + 인코딩 = 문코딩

1) 아스키 코드(128개 표현가능)

- 아스키 문자들 십육진수 -> 이진수 

- 코드 포인트 : 문자에 부여된 값

- 여기있는거말고는 표현 불가능

 

2) EUC-KR(2350개 가능)

- 한글의 완성형 인코딩 방식 

- 단점 : 문자집합에 정의되지 않은 글자는 표현 못함 = 모든 한글 불가능 

언어별로 언제 다 표현하고있어 -_- 통일된 거 없어? 있다! 뭐? 

3) Unicode UTF-8 

유니코드

- 통일된 문자 집합 

- 거의 다 표현 가능

 UTF-8 

- 유니코드 인코딩 방법 중 하나 
- 
